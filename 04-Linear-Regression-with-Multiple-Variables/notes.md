## 多个变量的线性回归
### 基本形式
![equation](./img/01.png)

为了记号方便，令![x0](./img/02.png)，

并令![matrix](./img/04.png)，![matrix](./img/03.png)，

则有：![matrix](./img/05.png)

最后这种形式的线性回归又被称为`Multivariate linear regression`

### 梯度下降法在多个变量的线性回归中的应用
Repeat {    
	![equation](./img/06.png)    
	(simultaneously update ![theta_j](./img/07.png)for j=0,...,n)    
}

### 多变量线性回归中一些有用的技巧
#### 让![x_i](./img/08.png)处于大致相同的数值范围
这样，![J_theta](./img/09.png)会大致成为一个圆形的碗（以两个变量为例），而不是椭圆形的碗，能加快算法的收敛。

根据实际经验，尽量让每个变量都处于![equation](./img/10.png)。
但这不是固定的，例如在0和3之间、-2和0.5之间，范围均相差不大，但是如果一个变量的变化范围在-100和100之间，就需要对这个变量做一些处理。当然也不能太小，例如在-0.000001和0.0000001之间也需要做一些处理。

通常处理的方法为：    
![equation](./img/11.png)    
其中：

- ![mu_i](./img/12.png)为训练集中![x_i](./img/13.png)的平均值   
- ![S_i](./img/14.png)为训练集中![x_i](./img/13.png)的变化范围（max - min）

#### 算法的调试 & 关于学习率![alpha](./img/15.png)
如果算法正确，那么每一次迭代后，![J_theta](./img/09.png)都会变小。

如果某一次迭代![J_theta](./img/09.png)减少的值小于![10^-3](./img/16.png)，则算法可以宣告收敛。

总结![alpha](./img/15.png)：

- 如果![alpha](./img/15.png)太小，则算法可能收敛的非常慢，经过超多次迭代才收敛
- 如果![alpha](./img/15.png)太大，则算法可能会越过最小值，可能会出现随着算法迭代，![J_theta](./img/09.png)变大的情况

根据实际经验，在选择![alpha](./img/15.png)时，可以尝试..., 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1...等等


