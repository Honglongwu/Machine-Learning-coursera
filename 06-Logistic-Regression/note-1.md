### Cost Function

回顾一下，在Logistic回归中，与线性回归类似，我们已知了其假设函数![02](./img/02.png)的形式，要根据训练集确定![02](./img/02.png)的参数![08](./img/08.png)。那么怎么确定呢？我们是通过代价函数Cost Function。

在线性回归中，我们的代价函数如下所示：

![30](./img/30.png)

令：

![31](./img/31.png)

则线性回归的代价函数为：

![32](./img/32.png)。

而在Logistic回归中，如果我们用同样的代价函数，由于![02](./img/02.png)是一个较为复杂的非线性函数，我们会发现Cost函数不是一个凸函数（non-convex），因此通过梯度下降算法找到的可能不是最小值而是一个局部极小值。

在Logistic回归中采用的Cost Function如下所示：

![33](./img/33.png)

当y=1时，函数为：

<img src="./img/34.png" width="450px"/>

只观察这一个函数，我们可以发现以下规律：

当![35](./img/35.png)时，说明![02](./img/02.png)做出了精确的预测，此时Cost的值为0；

但是当：![36](./img/36.png)时，![37](./img/37.png)，因为![38](./img/38.png)，也就是说实际的y值为1，结果我们的假设函数做出了0的预测，代价显然是无穷大。

当y=0时，函数图像为：

<img src="./img/39.png" width="450px"/>

我们可以发现与y=1时相类似的规律。




